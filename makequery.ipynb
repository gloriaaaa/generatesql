{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import textwrap\n",
    "import time\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "from mako import template\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psycopg2 as pg\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from neurocard_lib import common,datasets,join_utils,utils\n",
    "from neurocard_lib.factorized_sampler import FactorizedSamplerIterDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeTablesKey(table_names):\n",
    "    sorted_tables = sorted(table_names)\n",
    "    return '-'.join(sorted_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeQueries(num_queries, tables_in_templates, table_names,join_keys, rng):\n",
    "    MIN_filters=1\n",
    "    MAX_filters=8\n",
    "    \"\"\"Sample a tuple from actual join result then place filters.\"\"\"\n",
    "    # spark.catalog.clearCache()\n",
    "     # TODO: this assumes single equiv class.\n",
    "    join_items = list(join_keys.items())\n",
    "    lhs = join_items[0][1]\n",
    "    join_clauses_list = []\n",
    "    for rhs in join_items[1:]:\n",
    "        rhs = rhs[1]\n",
    "        join_clauses_list.append('{} = {}'.format(lhs, rhs))\n",
    "        lhs = rhs\n",
    "    join_clauses = '\\n AND '.join(join_clauses_list)\n",
    "\n",
    "    # Take only the content columns.\n",
    "    content_cols = []\n",
    "    categoricals = []\n",
    "    numericals = []\n",
    "    for table_name in table_names:\n",
    "        # categorical_cols = datasets.JoinOrderBenchmark.CATEGORICAL_COLUMNS[table_name]\n",
    "        # for c in categorical_cols:\n",
    "        #     disambiguated_name = common.JoinTableAndColumnNames(table_name, c,sep='.')\n",
    "        #     content_cols.append(disambiguated_name)\n",
    "        #     categoricals.append(disambiguated_name)\n",
    "\n",
    "        range_cols = datasets.JoinOrderBenchmark.RANGE_COLUMNS[table_name]\n",
    "        for c in range_cols:\n",
    "            disambiguated_name = common.JoinTableAndColumnNames(table_name,c,sep='.')\n",
    "            content_cols.append(disambiguated_name)\n",
    "            numericals.append(disambiguated_name)\n",
    "    \n",
    "    # Build a concat table reprsesenting the join result schema.\n",
    "    join_keys_list = [join_keys[n] for n in table_names]\n",
    "    print(join_keys_list)\n",
    "    join_spec = join_utils.get_join_spec({\n",
    "        \"join_tables\": table_names,\n",
    "        \"join_keys\": dict(\n",
    "            zip(table_names, [[k.split(\".\")[1]] for k in join_keys_list])),\n",
    "        \"join_root\": \"users\",\n",
    "        \"join_how\": \"inner\",\n",
    "    })\n",
    "    ds = FactorizedSamplerIterDataset(tables_in_templates,\n",
    "                                      join_spec,\n",
    "                                      sample_batch_size=num_queries,\n",
    "                                      disambiguate_column_names=False,\n",
    "                                      add_full_join_indicators=False,\n",
    "                                      add_full_join_fanouts=False)\n",
    "    concat_table = common.ConcatTables(tables_in_templates,\n",
    "                                       join_keys_list,\n",
    "                                       sample_from_join_dataset=ds)\n",
    "\n",
    "    template_for_execution = template.Template(\n",
    "        textwrap.dedent(\"\"\"\n",
    "        SELECT COUNT(*)\n",
    "        FROM ${', '.join(table_names)}\n",
    "        WHERE ${join_clauses}\n",
    "        AND ${filter_clauses};\n",
    "    \"\"\").strip())\n",
    "\n",
    "    true_inner_join_card = ds.sampler.join_card\n",
    "    # true_full_join_card = JOB_LIGHT_OUTER_CARDINALITY\n",
    "    print('True inner join card', true_inner_join_card)\n",
    "\n",
    "    ncols = len(content_cols)\n",
    "    queries = []\n",
    "    filter_strings = []\n",
    "    sql_queries = []  # To get true cardinalities.\n",
    "\n",
    "    while len(queries) < num_queries:\n",
    "        sampled_df = ds.sampler.run()[content_cols]\n",
    "\n",
    "        for r in sampled_df.iterrows():\n",
    "            tup = r[1]\n",
    "            num_filters = rng.randint(MIN_filters, max(ncols // 2, MAX_filters))\n",
    "\n",
    "            # Positions where the values are non-null.\n",
    "            non_null_indices = np.argwhere(~pd.isnull(tup).values).reshape(-1,)\n",
    "            if len(non_null_indices) < num_filters:\n",
    "                continue\n",
    "            print('{} filters out of {} content cols'.format(\n",
    "                num_filters, ncols))\n",
    "\n",
    "            # Place {'<=', '>=', '='} on numericals and '=' on categoricals.\n",
    "            idxs = rng.choice(non_null_indices, replace=False, size=num_filters)\n",
    "            vals = tup[idxs].values\n",
    "            cols = np.take(content_cols, idxs)\n",
    "            ops = rng.choice(['<=', '>=', '='], size=num_filters)\n",
    "            sensible_to_do_range = [c in numericals for c in cols]\n",
    "            ops = np.where(sensible_to_do_range, ops, '=')\n",
    "\n",
    "            print('cols', cols, 'ops', ops, 'vals', vals)\n",
    "\n",
    "            queries.append((cols, ops, vals))\n",
    "            filter_strings.append(','.join(\n",
    "                [','.join((c, o, str(v))) for c, o, v in zip(cols, ops, vals)]))\n",
    "\n",
    "            # Quote string literals & leave other literals alone.\n",
    "            filter_clauses = '\\n AND '.join([\n",
    "                '{} {} {}'.format(col, op, val)\n",
    "                if concat_table[col].data.dtype in [np.int64, np.float64] else\n",
    "                '{} {} \\'{}\\''.format(col, op, val)\n",
    "                for col, op, val in zip(cols, ops, vals)\n",
    "            ])\n",
    "\n",
    "            sql = template_for_execution.render(table_names=table_names,\n",
    "                                                join_clauses=join_clauses,\n",
    "                                                filter_clauses=filter_clauses)\n",
    "            sql_queries.append(sql)\n",
    "\n",
    "            if len(queries) >= num_queries:\n",
    "                break\n",
    "\n",
    "    true_cards = []\n",
    "    for i, sql_query in enumerate(sql_queries):\n",
    "        print(sql_query)\n",
    "        # DropBufferCache()\n",
    "        # spark.catalog.clearCache()\n",
    "\n",
    "        # print('  Query',\n",
    "        #       i,\n",
    "        #       'out of',\n",
    "        #       len(sql_queries),\n",
    "        #       '[{}]'.format(filter_strings[i]),\n",
    "        #       end='')\n",
    "\n",
    "        # t1 = time.time()\n",
    "\n",
    "        # true_card = ExecuteSql(spark, sql_query)[0][0]\n",
    "\n",
    "        # cursor.execute(sql_query)\n",
    "        # result = cursor.fetchall()\n",
    "        # true_card = result[0][0]\n",
    "\n",
    "        # dur = time.time() - t1\n",
    "\n",
    "        # true_cards.append(true_card)\n",
    "        # print(\n",
    "        #     '...done: {} (inner join sel {}; full sel {}; inner join {}); dur {:.1f}s'\n",
    "        #     .format(true_card, true_card / true_inner_join_card,\n",
    "        #             true_card / true_full_join_card, true_inner_join_card, dur))\n",
    "\n",
    "        # if i > 0 and i % 1 == 0:\n",
    "        #     spark = StartSpark(spark)\n",
    "\n",
    "    # df = pd.DataFrame({\n",
    "    #     'tables': [','.join(table_names)] * len(true_cards),\n",
    "    #     'join_conds': [\n",
    "    #         ','.join(map(lambda s: s.replace(' ', ''), join_clauses_list))\n",
    "    #     ] * len(true_cards),\n",
    "    #     'filters': filter_strings,\n",
    "    #     'true_cards': true_cards,\n",
    "    # })\n",
    "    # df.to_csv(, sep='#', mode='a', index=False, header=False)\n",
    "    # print('Template done.')\n",
    "    return queries, true_cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset_util\n",
    "def main():\n",
    "    dataset_dir='data/STATS_NEW/'\n",
    "    queries_path='workload/stats_query_chosen.csv'\n",
    "    output_csv = 'workload/stats_gen.csv'\n",
    "    NUM_QUERIES=100\n",
    "\n",
    "    cursor = None\n",
    "    tables = dataset_util.LoadSTATS(data_dir=dataset_dir,use_cols=None)\n",
    "    print(\"Load tables.\")\n",
    "    queries = dataset_util.STATSToQuery(queries_path, use_alias_keys=False)\n",
    "    print(\"Load queries.\")\n",
    "    # print(queries)\n",
    "    tables_to_join_keys = {}\n",
    "    for query in queries:\n",
    "        key = MakeTablesKey(query[0])\n",
    "        if key not in tables_to_join_keys:\n",
    "            join_dict = query[1]\n",
    "            # Disambiguate: title->id changed to title->title.id.\n",
    "            for table_name in join_dict.keys():\n",
    "                # TODO: only support a single join key\n",
    "                join_key = next(iter(join_dict[table_name]))\n",
    "                join_dict[table_name] = common.JoinTableAndColumnNames(\n",
    "                    table_name, join_key, sep='.')\n",
    "            tables_to_join_keys[key] = join_dict\n",
    "\n",
    "    num_templates = len(tables_to_join_keys)\n",
    "    num_queries_per_template = NUM_QUERIES // num_templates\n",
    "    print(num_templates,'join templates.')\n",
    "\n",
    "    rng = np.random.RandomState(1234)\n",
    "    queries = []  # [(cols, ops, vals)]\n",
    "\n",
    "    # Disambiguate to not prune away stuff during join sampling.\n",
    "    for table_name, table in tables.items():\n",
    "        for col in table.columns:\n",
    "            col.name = common.JoinTableAndColumnNames(table.name,\n",
    "                                                      col.name,\n",
    "                                                      sep='.')\n",
    "        table.data.columns = [col.name for col in table.columns]\n",
    "\n",
    "    # Generate queries.\n",
    "    last_run_queries = file_len(output_csv) if os.path.exists(output_csv) else 0\n",
    "    next_template_idx = last_run_queries // num_queries_per_template\n",
    "    print('next_template_idx', next_template_idx)\n",
    "    print(tables_to_join_keys.items())\n",
    "\n",
    "    # spark = StartSpark()\n",
    "    for i, (tables_to_join, join_keys) in enumerate(tables_to_join_keys.items()):\n",
    "\n",
    "        if i < next_template_idx:\n",
    "            print('Skipping template:', tables_to_join)\n",
    "            continue\n",
    "        print('Template:', tables_to_join)\n",
    "\n",
    "        if i == num_templates - 1:\n",
    "            num_queries_per_template += NUM_QUERIES % num_templates\n",
    "\n",
    "        # Generate num_queries_per_template.\n",
    "        table_names = tables_to_join.split('-')\n",
    "\n",
    "        tables_in_templates = [tables[n] for n in table_names]\n",
    "\n",
    "        queries.extend(MakeQueries(num_queries_per_template, tables_in_templates, table_names, join_keys, rng))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def choose_queries(queries_path,use_alias_keys=True):\n",
    "    sql_file = open(queries_path)\n",
    "    queries = sql_file.readlines()\n",
    "    sql_file.close()\n",
    " \n",
    "\n",
    "    chosensql=[]\n",
    "    for line in queries:\n",
    "        # data_raw=line.split(\"#\")\n",
    "        table_dict=line.split(\"#\")[0].split(',')\n",
    "        if len(table_dict)<=3:\n",
    "            chosensql.append(line)\n",
    "    \n",
    "    f=open(queries_path[:-4]+\"_chosen.csv\",\"w\")\n",
    "    f.writelines(chosensql)\n",
    "    f.close()\n",
    "    # with open(queries_path) as f:\n",
    "    #     data=\n",
    "    #     data_raw = list(list(rec) for rec in csv.reader(f, delimiter='#'))\n",
    "    #     for row in data_raw:\n",
    "    #         reader = csv.reader(row)  # comma-separated\n",
    "    #         table_dict =utils._get_table_dict(next(reader))\n",
    "    #         join_dict =utils._get_join_dict(next(reader), table_dict, use_alias_keys)\n",
    "    #         # print(len(join_dict.keys()))\n",
    "    #         if len(join_dict.keys())<=3:\n",
    "    #             print(row)\n",
    "\n",
    "# choose_queries('workload/stats_sub_query.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded parsed Table from data/STATS_NEW/badges.table\n",
      "Loaded parsed Table from data/STATS_NEW/votes.table\n",
      "Loaded parsed Table from data/STATS_NEW/postHistory.table\n",
      "Loaded parsed Table from data/STATS_NEW/posts.table\n",
      "Loaded parsed Table from data/STATS_NEW/users.table\n",
      "Loaded parsed Table from data/STATS_NEW/comments.table\n",
      "Loaded parsed Table from data/STATS_NEW/postLinks.table\n",
      "Loaded parsed Table from data/STATS_NEW/tags.table\n",
      "Load tables.\n",
      "Load queries.\n",
      "17 join templates.\n",
      "next_template_idx 0\n",
      "dict_items([('badges-users', defaultdict(<class 'set'>, {'badges': 'badges.UserId', 'users': 'users.Id'})), ('badges-comments', defaultdict(<class 'set'>, {'comments': 'comments.UserId', 'badges': 'badges.UserId'})), ('comments-postHistory', defaultdict(<class 'set'>, {'comments': 'comments.UserId', 'postHistory': 'postHistory.UserId'})), ('comments-votes', defaultdict(<class 'set'>, {'comments': 'comments.UserId', 'votes': 'votes.UserId'})), ('badges-posts', defaultdict(<class 'set'>, {'badges': 'badges.UserId', 'posts': 'posts.OwnerUserId'})), ('comments-postLinks-posts', defaultdict(<class 'set'>, {'comments': 'comments.UserId', 'posts': 'posts.Id', 'postLinks': 'postLinks.PostId'})), ('comments-postHistory-posts', defaultdict(<class 'set'>, {'posts': 'posts.Id', 'comments': 'comments.PostId', 'postHistory': 'postHistory.PostId'})), ('comments-posts-users', defaultdict(<class 'set'>, {'comments': 'comments.UserId', 'users': 'users.Id', 'posts': 'posts.OwnerUserId'})), ('comments-postHistory-users', defaultdict(<class 'set'>, {'users': 'users.Id', 'comments': 'comments.UserId', 'postHistory': 'postHistory.UserId'})), ('comments-users-votes', defaultdict(<class 'set'>, {'users': 'users.Id', 'comments': 'comments.UserId', 'votes': 'votes.UserId'})), ('badges-comments-users', defaultdict(<class 'set'>, {'users': 'users.Id', 'comments': 'comments.UserId', 'badges': 'badges.UserId'})), ('posts-tags-votes', defaultdict(<class 'set'>, {'posts': 'posts.OwnerUserId', 'tags': 'tags.ExcerptPostId', 'votes': 'votes.UserId'})), ('postHistory-postLinks-posts', defaultdict(<class 'set'>, {'posts': 'posts.Id', 'postLinks': 'postLinks.PostId', 'postHistory': 'postHistory.PostId'})), ('postLinks-posts-users', defaultdict(<class 'set'>, {'posts': 'posts.OwnerUserId', 'postLinks': 'postLinks.PostId', 'users': 'users.Id'})), ('postHistory-posts-users', defaultdict(<class 'set'>, {'postHistory': 'postHistory.PostId', 'posts': 'posts.OwnerUserId', 'users': 'users.Id'})), ('posts-users-votes', defaultdict(<class 'set'>, {'votes': 'votes.UserId', 'posts': 'posts.Id', 'users': 'users.Id'})), ('badges-users-votes', defaultdict(<class 'set'>, {'users': 'users.Id', 'votes': 'votes.UserId', 'badges': 'badges.UserId'}))])\n",
      "Template: badges-users\n",
      "['badges.UserId', 'users.Id']\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "['badges', 'users']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10055/451043146.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_10055/4005731448.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mtables_in_templates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtable_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mqueries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMakeQueries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_queries_per_template\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtables_in_templates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoin_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_10055/3244648908.py\u001b[0m in \u001b[0;36mMakeQueries\u001b[0;34m(num_queries, tables_in_templates, table_names, join_keys, rng)\u001b[0m\n\u001b[1;32m     39\u001b[0m             zip(table_names, [[k.split(\".\")[1]] for k in join_keys_list])),\n\u001b[1;32m     40\u001b[0m         \u001b[0;34m\"join_root\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"posts\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;34m\"join_how\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"inner\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     })\n\u001b[1;32m     43\u001b[0m     ds = FactorizedSamplerIterDataset(tables_in_templates,\n",
      "\u001b[0;32m/mnt/mysqlstorage/generatesql/neurocard_lib/join_utils.py\u001b[0m in \u001b[0;36mget_join_spec\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     83\u001b[0m         join_clauses = _infer_join_clauses(config[\"join_tables\"],\n\u001b[1;32m     84\u001b[0m                                            \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"join_keys\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                                            config[\"join_root\"])\n\u001b[0m\u001b[1;32m     86\u001b[0m     \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_join_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin_clauses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"join_root\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     join_hash = hashlib.sha1(\n",
      "\u001b[0;32m/mnt/mysqlstorage/generatesql/neurocard_lib/join_utils.py\u001b[0m in \u001b[0;36m_infer_join_clauses\u001b[0;34m(tables, join_keys, t0)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjoin_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoin_keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mt0\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0mt0_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mk0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoin_keys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: ['badges', 'users']"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a4d6fe06ae33849d805fbd4bdd134eb7ae3985d67141138b171cfdeb1321d3b0"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('neurocard': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
